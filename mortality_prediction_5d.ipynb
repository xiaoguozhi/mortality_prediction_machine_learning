{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir(\"D:/data/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>51</th>\n",
       "      <th>184</th>\n",
       "      <th>190</th>\n",
       "      <th>211</th>\n",
       "      <th>442</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>676</th>\n",
       "      <th>678</th>\n",
       "      <th>...</th>\n",
       "      <th>226567</th>\n",
       "      <th>226584</th>\n",
       "      <th>227488</th>\n",
       "      <th>227489</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>label1-2d</th>\n",
       "      <th>label2-5d</th>\n",
       "      <th>label3-10d</th>\n",
       "      <th>label4-30d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>100001</td>\n",
       "      <td>117.786058</td>\n",
       "      <td>4</td>\n",
       "      <td>0.582271</td>\n",
       "      <td>99.312406</td>\n",
       "      <td>119.392324</td>\n",
       "      <td>6</td>\n",
       "      <td>118.497302</td>\n",
       "      <td>36.831826</td>\n",
       "      <td>98.167991</td>\n",
       "      <td>...</td>\n",
       "      <td>542.34375</td>\n",
       "      <td>1231.078431</td>\n",
       "      <td>7776.896552</td>\n",
       "      <td>8641.809524</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>100003</td>\n",
       "      <td>117.786058</td>\n",
       "      <td>4</td>\n",
       "      <td>0.582271</td>\n",
       "      <td>99.312406</td>\n",
       "      <td>119.392324</td>\n",
       "      <td>6</td>\n",
       "      <td>118.497302</td>\n",
       "      <td>36.831826</td>\n",
       "      <td>98.167991</td>\n",
       "      <td>...</td>\n",
       "      <td>542.34375</td>\n",
       "      <td>1231.078431</td>\n",
       "      <td>7776.896552</td>\n",
       "      <td>8641.809524</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hadm_id          51  184       190        211         442  454         455  \\\n",
       "0   100001  117.786058    4  0.582271  99.312406  119.392324    6  118.497302   \n",
       "1   100003  117.786058    4  0.582271  99.312406  119.392324    6  118.497302   \n",
       "\n",
       "         676        678  ...     226567       226584       227488  \\\n",
       "0  36.831826  98.167991  ...  542.34375  1231.078431  7776.896552   \n",
       "1  36.831826  98.167991  ...  542.34375  1231.078431  7776.896552   \n",
       "\n",
       "        227489  age  gender  label1-2d  label2-5d  label3-10d  label4-30d  \n",
       "0  8641.809524   35       0          0          0           0           0  \n",
       "1  8641.809524   59       1          0          0           0           0  \n",
       "\n",
       "[2 rows x 65 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1列hadm_id,58列SAPS-II特征，1列年龄，1列性别，4列死亡标签，共65列\n",
    "df=pd.read_csv(\"feature_clean.csv\")\n",
    "del df['Unnamed: 0']\n",
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47642, 65)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47642"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.hadm_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1列hadm_id,58列SAPS-II特征，1列年龄，1列性别，4列死亡标签，共65列\n",
    "#分别取出数据集中的自变量和因变量\n",
    "#先用患者入院后第2天的死亡率作为预测变量\n",
    "X=df.iloc[:,1:61]\n",
    "y=df.iloc[:,62]\n",
    "XX=X.values\n",
    "yy=y.values\n",
    "#记录数据框的列名\n",
    "col_name=pd.concat([X,y],axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statistics import mean\n",
    "from imblearn.combine import SMOTEENN \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def kfold_imb( XX, yy, model):\n",
    "    cv = StratifiedKFold(n_splits=5,shuffle=False,random_state=None)\n",
    "    \n",
    "    acc_total=[]\n",
    "    prec_total=[]\n",
    "    reca_total=[]\n",
    "    f1_total=[]\n",
    "    auc_total=[]\n",
    "    prc_total=[]\n",
    "    ks_total=[]\n",
    "    for train, test in cv.split(XX,yy):\n",
    "        #过采样\n",
    "        smote_enn = SMOTEENN(random_state=1)\n",
    "        X_resampled, y_resampled = smote_enn.fit_sample(XX[train],yy[train])\n",
    "        model.fit(X_resampled, y_resampled)\n",
    "        score = model.fit(X_resampled, y_resampled).predict_proba(XX[test])\n",
    "        preds = model.predict(XX[test])\n",
    "        \n",
    "        from sklearn.metrics import f1_score\n",
    "        acc=accuracy_score(yy[test], preds)\n",
    "        prec=precision_score(yy[test],preds)\n",
    "        reca=recall_score(yy[test], preds)\n",
    "        f1=f1_score(yy[test], preds)\n",
    "        auc=roc_auc_score(yy[test],score[:,1])\n",
    "        prc= average_precision_score(yy[test], score[:,1])\n",
    "        fpr, tpr, thresholds = roc_curve(yy[test], score[:, 1])\n",
    "        ks_value=max(abs(fpr-tpr))\n",
    "        \n",
    "        acc_total.append(acc)\n",
    "        prec_total.append(prec)\n",
    "        reca_total.append(reca)\n",
    "        f1_total.append(f1)\n",
    "        auc_total.append(auc)\n",
    "        prc_total.append(prc)\n",
    "        ks_total.append(ks_value)\n",
    "    accuracy=np.mean(acc_total)\n",
    "    precision=np.mean(prec_total)\n",
    "    recall=np.mean(reca_total)\n",
    "    f1_score=np.mean(f1_total)\n",
    "    auroc=np.mean(auc_total)\n",
    "    auprc=np.mean(prc_total)\n",
    "    ks=np.mean(ks_total)\n",
    "        #score = evaluation(yy[test], y_predict)\n",
    "        #score_total.append(score)\n",
    "    #score = np.mean(score_total)\n",
    "    mm=pd.DataFrame(data=[accuracy, precision,recall,f1_score,auroc,auprc,ks],\n",
    "                    index=[\"accuracy\", \"precision\", \"recall\", \"f1_score\",\"roc_auc_score\",\"prc_auc_score\",\"ks-value\"])\n",
    "    return mm\n",
    "\n",
    "def kfold( XX, yy, model):\n",
    "    cv = StratifiedKFold(n_splits=5,shuffle=False,random_state=None)\n",
    "    #kf = KFold(n_splits=4)\n",
    "    acc_total=[]\n",
    "    prec_total=[]\n",
    "    reca_total=[]\n",
    "    f1_total=[]\n",
    "    auc_total=[]\n",
    "    prc_total=[]\n",
    "    ks_total=[]\n",
    "    for train, test in cv.split(XX,yy):\n",
    "        model.fit(XX[train], yy[train])\n",
    "        preds = model.predict(XX[test])\n",
    "        score = model.fit(XX[train], yy[train]).predict_proba(XX[test])\n",
    "        from sklearn.metrics import f1_score\n",
    "        acc=accuracy_score(yy[test], preds)\n",
    "        prec=precision_score(yy[test],preds)\n",
    "        reca=recall_score(yy[test], preds)\n",
    "        f1=f1_score(yy[test], preds)\n",
    "        #auc=roc_auc_score(yy[test], preds)\n",
    "        ####修改后\n",
    "        auc=roc_auc_score(yy[test], score[:,1])\n",
    "        prc= average_precision_score(yy[test], score[:,1])\n",
    "        fpr, tpr, thresholds = roc_curve(yy[test], score[:, 1])\n",
    "        ks_value=max(abs(fpr-tpr))\n",
    "        \n",
    "        acc_total.append(acc)\n",
    "        prec_total.append(prec)\n",
    "        reca_total.append(reca)\n",
    "        f1_total.append(f1)\n",
    "        auc_total.append(auc)\n",
    "        prc_total.append(prc)\n",
    "        ks_total.append(ks_value)\n",
    "    \n",
    "    accuracy=np.mean(acc_total)\n",
    "    precision=np.mean(prec_total)\n",
    "    recall=np.mean(reca_total)\n",
    "    f1_score=np.mean(f1_total)\n",
    "    auroc=np.mean(auc_total)\n",
    "    auprc=np.mean(prc_total)\n",
    "    ks=np.mean(ks_total)\n",
    "    mm=pd.DataFrame(data=[accuracy, precision,recall,f1_score,auroc,auprc,ks],\n",
    "                    index=[\"accuracy\", \"precision\", \"recall\", \"f1_score\",\"roc_auc_score\",\"prc_auc_score\",\"ks-value\"])    \n",
    "    return mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imblance_logistic regression's accuracy:\n",
      "                      0\n",
      "accuracy       0.577789\n",
      "precision      0.094846\n",
      "recall         0.745693\n",
      "f1_score       0.168279\n",
      "roc_auc_score  0.731144\n",
      "prc_auc_score  0.172914\n",
      "ks-value       0.350998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression's accuracy:\n",
      "                      0\n",
      "accuracy       0.942005\n",
      "precision      0.320430\n",
      "recall         0.008428\n",
      "f1_score       0.016236\n",
      "roc_auc_score  0.719798\n",
      "prc_auc_score  0.162759\n",
      "ks-value       0.351147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "[LogisticRegression(C=1.0,random_state=None),\n",
    " DecisionTreeClassifier(criterion='gini',random_state=None),\n",
    " RandomForestClassifier(max_depth=None,random_state=None),\n",
    " KNeighborsClassifier(n_neighbors=5),\n",
    " GradientBoostingClassifier(n_estimators=100,max_features=None,max_depth=3,random_state=None),\n",
    " AdaBoostClassifier(n_estimators=50,random_state=None),\n",
    " LGBMClassifier(random_state=None),\n",
    " XGBClassifier()\n",
    "]\n",
    "\n",
    "lr=LogisticRegression(C=1.0,random_state=None)\n",
    "print(\"imblance_logistic regression's accuracy:\\n{}\".format(kfold_imb( XX, yy, lr)))    \n",
    "print(\"logistic regression's accuracy:\\n{}\".format(kfold( XX, yy, lr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imblance_decision tree's accuracy:\n",
      "                      0\n",
      "accuracy       0.863419\n",
      "precision      0.191837\n",
      "recall         0.430933\n",
      "f1_score       0.265451\n",
      "roc_auc_score  0.660315\n",
      "prc_auc_score  0.115629\n",
      "ks-value       0.320631\n",
      "decision tree's accuracy:\n",
      "                      0\n",
      "accuracy       0.913669\n",
      "precision      0.270595\n",
      "recall         0.299374\n",
      "f1_score       0.284056\n",
      "roc_auc_score  0.625565\n",
      "prc_auc_score  0.121686\n",
      "ks-value       0.251131\n"
     ]
    }
   ],
   "source": [
    "#decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT=DecisionTreeClassifier(criterion='gini',random_state=None)\n",
    "print(\"imblance_decision tree's accuracy:\\n{}\".format(kfold_imb( XX, yy, DT)))\n",
    "print(\"decision tree's accuracy:\\n{}\".format(kfold( XX, yy, DT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imblance_RandomForest's accuracy:\n",
      "                      0\n",
      "accuracy       0.919252\n",
      "precision      0.346607\n",
      "recall         0.461350\n",
      "f1_score       0.395653\n",
      "roc_auc_score  0.860330\n",
      "prc_auc_score  0.348112\n",
      "ks-value       0.561353\n",
      "RandomForest's accuracy:\n",
      "                      0\n",
      "accuracy       0.950842\n",
      "precision      0.837621\n",
      "recall         0.175893\n",
      "f1_score       0.290451\n",
      "roc_auc_score  0.856357\n",
      "prc_auc_score  0.451003\n",
      "ks-value       0.563797\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RF=RandomForestClassifier(max_depth=None,random_state=None)\n",
    "print(\"imblance_RandomForest's accuracy:\\n{}\".format(kfold_imb( XX, yy, RF)))\n",
    "print(\"RandomForest's accuracy:\\n{}\".format(kfold( XX, yy, RF)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imblance_KNN's accuracy:\n",
      "                      0\n",
      "accuracy       0.730595\n",
      "precision      0.114328\n",
      "recall         0.548554\n",
      "f1_score       0.189213\n",
      "roc_auc_score  0.679298\n",
      "prc_auc_score  0.107315\n",
      "ks-value       0.308154\n",
      "KNN's accuracy:\n",
      "                      0\n",
      "accuracy       0.942236\n",
      "precision      0.459248\n",
      "recall         0.055698\n",
      "f1_score       0.099198\n",
      "roc_auc_score  0.632636\n",
      "prc_auc_score  0.127253\n",
      "ks-value       0.249131\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN=KNeighborsClassifier(n_neighbors=5)\n",
    "print(\"imblance_KNN's accuracy:\\n{}\".format(kfold_imb( XX, yy, KNN)))\n",
    "print(\"KNN's accuracy:\\n{}\".format(kfold( XX, yy, KNN)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imblance_GBDT's accuracy:\n",
      "                      0\n",
      "accuracy       0.876643\n",
      "precision      0.245724\n",
      "recall         0.557353\n",
      "f1_score       0.340990\n",
      "roc_auc_score  0.842870\n",
      "prc_auc_score  0.308504\n",
      "ks-value       0.529764\n",
      "GBDT's accuracy:\n",
      "                      0\n",
      "accuracy       0.950065\n",
      "precision      0.733841\n",
      "recall         0.201182\n",
      "f1_score       0.315552\n",
      "roc_auc_score  0.863361\n",
      "prc_auc_score  0.438068\n",
      "ks-value       0.571880\n"
     ]
    }
   ],
   "source": [
    "#GBDT\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBDT=GradientBoostingClassifier(n_estimators=100,max_features=None,max_depth=3,random_state=None)\n",
    "print(\"imblance_GBDT's accuracy:\\n{}\".format(kfold_imb( XX, yy, GBDT)))\n",
    "print(\"GBDT's accuracy:\\n{}\".format(kfold( XX, yy, GBDT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imblance_AdaBoost's accuracy:\n",
      "                      0\n",
      "accuracy       0.846396\n",
      "precision      0.198744\n",
      "recall         0.554430\n",
      "f1_score       0.292543\n",
      "roc_auc_score  0.813264\n",
      "prc_auc_score  0.251393\n",
      "ks-value       0.476715\n",
      "AdaBoost's accuracy:\n",
      "                      0\n",
      "accuracy       0.946770\n",
      "precision      0.597346\n",
      "recall         0.221698\n",
      "f1_score       0.322861\n",
      "roc_auc_score  0.851931\n",
      "prc_auc_score  0.386605\n",
      "ks-value       0.546180\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Ada=AdaBoostClassifier(n_estimators=50,random_state=None)\n",
    "print(\"imblance_AdaBoost's accuracy:\\n{}\".format(kfold_imb( XX, yy, Ada)))\n",
    "print(\"AdaBoost's accuracy:\\n{}\".format(kfold( XX, yy, Ada)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imblance_lightgbm's accuracy:\n",
      "                      0\n",
      "accuracy       0.915369\n",
      "precision      0.337636\n",
      "recall         0.495057\n",
      "f1_score       0.401355\n",
      "roc_auc_score  0.859476\n",
      "prc_auc_score  0.378765\n",
      "ks-value       0.552998\n",
      "lightgbm's accuracy:\n",
      "                      0\n",
      "accuracy       0.951262\n",
      "precision      0.706267\n",
      "recall         0.256514\n",
      "f1_score       0.375816\n",
      "roc_auc_score  0.877147\n",
      "prc_auc_score  0.465020\n",
      "ks-value       0.594606\n"
     ]
    }
   ],
   "source": [
    "#lightGBM\n",
    "import lightgbm.LGBMClassifier as LGBMClassifier\n",
    "lgb=LGBMClassifier(random_state=None)\n",
    "print(\"imblance_lightgbm's accuracy:\\n{}\".format(kfold_imb( XX, yy, lgb)))\n",
    "print(\"lightgbm's accuracy:\\n{}\".format(kfold( XX, yy, lgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:10:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:10:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:11:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:11:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:13:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:13:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:14:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:14:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:16:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "imblance_xgboost's accuracy:\n",
      "                      0\n",
      "accuracy       0.922673\n",
      "precision      0.363987\n",
      "recall         0.467574\n",
      "f1_score       0.409202\n",
      "roc_auc_score  0.855332\n",
      "prc_auc_score  0.383827\n",
      "ks-value       0.544273\n",
      "[18:16:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:16:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:16:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:16:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda\\path\\lib\\site-packages\\xgboost\\sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:16:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "xgboost's accuracy:\n",
      "                      0\n",
      "accuracy       0.949960\n",
      "precision      0.657598\n",
      "recall         0.264572\n",
      "f1_score       0.377057\n",
      "roc_auc_score  0.867586\n",
      "prc_auc_score  0.446422\n",
      "ks-value       0.566223\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "from xgboost import XGBClassifier\n",
    "xgb=XGBClassifier()\n",
    "print(\"imblance_xgboost's accuracy:\\n{}\".format(kfold_imb( XX, yy, xgb)))\n",
    "print(\"xgboost's accuracy:\\n{}\".format(kfold( XX, yy, xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
